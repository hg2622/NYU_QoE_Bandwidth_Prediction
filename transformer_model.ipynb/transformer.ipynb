{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"/content/drive/My Drive/7Train1.csv\", header=None)\n",
    "bandwidth = data.iloc[:, 0].values  # Convert to NumPy array\n",
    "length = len(bandwidth)\n",
    "mean = np.mean(bandwidth)\n",
    "len_train = math.floor(length * 0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "data_tensor = torch.FloatTensor(bandwidth).view(-1, 1)\n",
    "\n",
    "# Prediction size\n",
    "predict_size = 10\n",
    "\n",
    "# Function to create in-out sequences\n",
    "def create_inout_sequences(input_data, window_size, predict_size):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L - window_size - predict_size + 1):\n",
    "        train_seq = input_data[i:i + window_size]\n",
    "        train_label = input_data[i + window_size:i + window_size + predict_size]\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return inout_seq\n",
    "\n",
    "# Parameters\n",
    "window_size = 30\n",
    "batch_size = 8\n",
    "\n",
    "# Create sequences for training from the first part of the data\n",
    "train_inout_seq = create_inout_sequences(data_tensor[:len_train], window_size, predict_size)\n",
    "\n",
    "# Custom dataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataset = TimeSeriesDataset(train_inout_seq)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# Create sequences for testing from the remaining part of the data\n",
    "test_inout_seq = create_inout_sequences(data_tensor[len_train:], window_size, predict_size)\n",
    "test_dataset = TimeSeriesDataset(test_inout_seq)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=True)\n",
    "\n",
    "# Define Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size=1, embed_dim=10, num_layers=2, hidden_dim=128, num_heads=2, output_size=1, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, output_size)\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask=None):\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        memory = self.transformer_encoder(src)\n",
    "\n",
    "        tgt = self.embedding(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TransformerModel(dropout=0.2, num_layers=2)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "epochs = 25\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for seq, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        seq = seq.view(window_size, batch_size, -1)\n",
    "        labels = labels.view(predict_size, batch_size, -1)\n",
    "\n",
    "        tgt = torch.cat((seq[-1:], labels[:-1]), dim=0)\n",
    "        tgt_mask = model.generate_square_subsequent_mask(tgt.size(0))\n",
    "\n",
    "        y_pred = model(seq, tgt, tgt_mask)\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += single_loss.item()\n",
    "\n",
    "    train_losses.append(epoch_train_loss / len(train_loader))\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {single_loss.item():.8f}')\n",
    "\n",
    "# Making predictions\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "for test_seq, _ in test_loader:\n",
    "    test_seq = test_seq.view(window_size, 1, -1)\n",
    "    pred_seq = test_seq[-1:]\n",
    "\n",
    "    for i in range(predict_size):\n",
    "        tgt_mask = model.generate_square_subsequent_mask(pred_seq.size(0))\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(test_seq, pred_seq, tgt_mask)\n",
    "            next_pred = y_pred[i]  # Take the prediction at the current time step\n",
    "            next_pred = next_pred.view(1, 1, -1)  # Reshape to match pred_seq dimensions\n",
    "            pred_seq = torch.cat((pred_seq, next_pred), dim=0)  # Append the prediction to the sequence\n",
    "\n",
    "    predictions.append(next_pred.item())\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "\n",
    "actual_values = bandwidth[len_train + window_size + predict_size-1:length]\n",
    "\n",
    "mae = mean_absolute_error(actual_values, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(actual_values, predictions))\n",
    "\n",
    "mean_actual = np.mean(actual_values)\n",
    "error_ratio_rmse = (rmse / mean_actual) * 100\n",
    "error_ratio_mae = (mae / mean_actual) * 100\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "print(f'Error Ratio RMSE: {error_ratio_rmse:.4f}%')\n",
    "print(f'Error Ratio MAE: {error_ratio_mae:.4f}%')\n",
    "\n",
    "predictions_195 = predictions[:195]\n",
    "actual_values_195 = actual_values[:195]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len_train + window_size, len_train + window_size + 195), actual_values_195, label='Actual Data')\n",
    "plt.plot(range(len_train + window_size, len_train + window_size + 195), predictions_195, label='Transformer Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Bandwidth\")\n",
    "plt.title(\"Transformer Predictions vs Actual Data (First 195 values from 3000 onwards)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
