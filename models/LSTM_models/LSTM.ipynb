{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"7Train1.csv\", header=None)\n",
    "bandwidth = data.iloc[:, 0].values  # Convert to NumPy array\n",
    "length = len(bandwidth)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "data_tensor = torch.FloatTensor(bandwidth).view(-1, 1)\n",
    "\n",
    "# Function to create in-out sequences\n",
    "def create_inout_sequences(input_data, window_size):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L - window_size):\n",
    "        train_seq = input_data[i:i + window_size]\n",
    "        train_label = input_data[i + window_size:i + window_size + 1]\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return inout_seq\n",
    "\n",
    "# Parameters\n",
    "window_size = 5\n",
    "\n",
    "# Create sequences for training from the first 3000 data points and last part after 3992\n",
    "train_inout_seq = create_inout_sequences(data_tensor[982:], window_size) \n",
    "\n",
    "# Create sequences for testing from 3000 to 3992\n",
    "test_inout_seq = create_inout_sequences(data_tensor[0:982], window_size)\n",
    "\n",
    "# Define LSTM model with adjustable number of layers and dropout\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=128, output_size=1, num_layers=2, dropout=0.5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        lstm_out, hidden_state = self.lstm(input_seq, hidden_state)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1], hidden_state\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.num_layers, 1, self.hidden_layer_size),\n",
    "                torch.zeros(self.num_layers, 1, self.hidden_layer_size))\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTM(dropout=0.6, num_layers=2)  # Adjust the dropout rate and number of layers as needed\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "epochs = 15\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hidden_state = model.init_hidden()  # Initialize hidden state for each epoch\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    for seq, labels in train_inout_seq:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        seq = seq.view(-1, 1, 1)  # Reshape for LSTM input\n",
    "        labels = labels.view(-1, 1)\n",
    "\n",
    "        y_pred, hidden_state = model(seq, hidden_state)\n",
    "\n",
    "        # Detach hidden state to prevent backpropagating through the entire history\n",
    "        hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += single_loss.item()\n",
    "\n",
    "    train_losses.append(epoch_train_loss / len(train_inout_seq))\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {single_loss.item():.8f}')\n",
    "\n",
    "# Making predictions\n",
    "model.eval()\n",
    "hidden_state = model.init_hidden()\n",
    "\n",
    "predictions = []\n",
    "for seq, _ in test_inout_seq:\n",
    "    seq = seq.view(-1, 1, 1)  # Reshape for LSTM input\n",
    "    with torch.no_grad():\n",
    "        y_pred, hidden_state = model(seq, hidden_state)\n",
    "        hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "        predictions.append(y_pred.item())\n",
    "\n",
    "# Convert predictions to numpy array\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Calculate actual values\n",
    "actual_values = bandwidth[window_size:982]\n",
    "\n",
    "# Calculate MAE and RMSE for all test data\n",
    "mae = mean_absolute_error(actual_values, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(actual_values, predictions))\n",
    "\n",
    "# Calculate error ratio as defined\n",
    "mean_actual = np.mean(actual_values)\n",
    "error_ratio_rmse = (rmse / mean_actual) * 100\n",
    "error_ratio_mae = (mae / mean_actual) * 100\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "print(f'Error Ratio RMSE: {error_ratio_rmse:.4f}%')\n",
    "print(f'Error Ratio MAE: {error_ratio_mae:.4f}%')\n",
    "\n",
    "# Slice to get some 195 values for plotting\n",
    "predictions_195 = predictions[:195]\n",
    "actual_values_195 = actual_values[:195]\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(3000 + window_size, 3000 + window_size + 195), actual_values_195, label='Actual Data')\n",
    "plt.plot(range(3000 + window_size, 3000 + window_size + 195), predictions_195, label='Predicted Data')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Bandwidth\")\n",
    "plt.title(\"LSTM Predictions vs Actual Data (First 195 values from 3000 onwards)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
