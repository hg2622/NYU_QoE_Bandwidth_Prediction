{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from google.colab import drive\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"/content/drive/My Drive/bus57.csv\", header=None)\n",
    "bandwidth = data.iloc[:, 0].values  # Convert to NumPy array\n",
    "length = len(bandwidth)\n",
    "mean = np.mean(bandwidth)\n",
    "len_train = math.floor(length * 0.8)\n",
    "\n",
    "for i in range(length):\n",
    "  if bandwidth[i] > 40:\n",
    "     bandwidth[i] = 0\n",
    "\n",
    "  \n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "bandwidth_normalized = scaler.fit_transform(bandwidth.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "data_tensor = torch.FloatTensor(bandwidth_normalized).view(-1, 1)\n",
    "\n",
    "# prediction size\n",
    "predict_size = 1\n",
    "\n",
    "# Function to create in-out sequences\n",
    "def create_inout_sequences(input_data, window_size, predict_size):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L - window_size - predict_size + 1):\n",
    "        train_seq = input_data[i:i + window_size]\n",
    "        train_label = input_data[i + window_size:i + window_size + predict_size]\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return inout_seq\n",
    "\n",
    "# Parameters\n",
    "window_size = 5\n",
    "\n",
    "# Custom dataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "# Create sequences for training from the first part of the data\n",
    "train_inout_seq = create_inout_sequences(data_tensor[:len_train], window_size, predict_size)\n",
    "\n",
    "# Create sequences for testing from the remaining part of the data\n",
    "test_inout_seq = create_inout_sequences(data_tensor[len_train:], window_size, predict_size)\n",
    "\n",
    "# Create DataLoader for training\n",
    "batch_size = 1\n",
    "train_dataset = TimeSeriesDataset(train_inout_seq)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Define the attention mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_layer_size, out_channel):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.linear = nn.Linear(out_channel, hidden_layer_size)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        query=query.transpose(0,1)\n",
    "        scores = torch.bmm(self.linear(key), query.transpose(1, 2))\n",
    "        attn_weights = self.sigmoid(scores)\n",
    "        context = torch.bmm(attn_weights.transpose(1, 2), value)\n",
    "        return context, attn_weights\n",
    "\n",
    "# Define LSTM model with attention\n",
    "class LSTMWithAttention(nn.Module): \n",
    "    def __init__(self, input_size=1, hidden_layer_size=64, output_size=predict_size, num_layers=2, dropout=0.2, cnn_kernel_size=window_size, memory_size=16, out_channel=64):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.out_channel = out_channel\n",
    "        self.num_layers = num_layers\n",
    "        self.memory_size = memory_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, dropout=dropout)\n",
    "        self.attention = Attention(hidden_layer_size, out_channel)\n",
    "        self.cnn = nn.Linear(memory_size, out_channel)\n",
    "        self.linear1 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.linear2 = nn.Linear(out_channel, hidden_layer_size)\n",
    "        self.linear3 = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_state, past_hidden_states):\n",
    "        lstm_out, hidden_state = self.lstm(input_seq, hidden_state)\n",
    "        past_hidden_states = torch.cat((past_hidden_states, lstm_out[-1].unsqueeze(0)), dim=0)\n",
    "\n",
    "        if past_hidden_states.size(0) > self.memory_size:\n",
    "            past_hidden_states = past_hidden_states[-self.memory_size:]\n",
    "\n",
    "        cnn_out = self.cnn(past_hidden_states.transpose(0, 1).transpose(1, 2))\n",
    "\n",
    "        query = lstm_out[-1].unsqueeze(0)\n",
    "        context, _ = self.attention(query, cnn_out, cnn_out)\n",
    "        predictions = self.linear2(context) + self.linear1(query)\n",
    "        predictions = self.linear3(predictions)\n",
    "        return predictions, hidden_state, past_hidden_states\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_layer_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_layer_size))\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMWithAttention(dropout=0.2, num_layers=3)  # Adjust the dropout rate and number of layers as needed\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# Training the model\n",
    "epochs = 30\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hidden_state = model.init_hidden(batch_size)  # Initialize hidden state for each epoch\n",
    "    past_hidden_states = torch.zeros(model.memory_size, batch_size, model.hidden_layer_size)  # Initialize past hidden states\n",
    "\n",
    "    epoch_train_loss = 0\n",
    "    for seq, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        seq = seq.view(window_size, batch_size, 1)  # Reshape for LSTM input\n",
    "        labels = labels.view(batch_size, -1)\n",
    "\n",
    "        y_pred, hidden_state, past_hidden_states = model(seq, hidden_state, past_hidden_states)\n",
    "\n",
    "        # Detach hidden state to prevent backpropagating through the entire history\n",
    "        hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "        past_hidden_states = past_hidden_states.detach()\n",
    "\n",
    "        single_loss = loss_function(y_pred.squeeze(), labels.squeeze())\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += single_loss.item()\n",
    "\n",
    "    train_losses.append(epoch_train_loss / len(train_loader))\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_train_loss / len(train_loader):.8f}')\n",
    "\n",
    "# Making predictions\n",
    "model.eval()\n",
    "hidden_state = model.init_hidden(1)\n",
    "past_hidden_states = torch.zeros(model.memory_size, 1, model.hidden_layer_size)\n",
    "\n",
    "predictions = []\n",
    "for seq, _ in test_inout_seq:\n",
    "    seq = seq.view(-1, 1, 1)  # Reshape for LSTM input\n",
    "    with torch.no_grad():\n",
    "        y_pred, hidden_state, past_hidden_states = model(seq, hidden_state, past_hidden_states)\n",
    "        y_pred = y_pred.view(-1)\n",
    "        hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "        past_hidden_states = past_hidden_states.detach()\n",
    "        predictions.append(y_pred[predict_size-1].item())\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate actual values\n",
    "actual_values = bandwidth[len_train + window_size + predict_size - 1:]\n",
    "\n",
    "# Calculate MAE and RMSE for all test data\n",
    "mae = mean_absolute_error(actual_values, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(actual_values, predictions))\n",
    "\n",
    "# Calculate error ratio as defined\n",
    "mean_actual = np.mean(actual_values)\n",
    "error_ratio_rmse = (rmse / mean_actual) * 100\n",
    "error_ratio_mae = (mae / mean_actual) * 100\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "print(f'Error Ratio RMSE: {error_ratio_rmse:.4f}%')\n",
    "print(f'Error Ratio MAE: {error_ratio_mae:.4f}%')\n",
    "\n",
    "# Slice to get some 195 values for plotting\n",
    "predictions_195 = predictions[:195]\n",
    "actual_values_195 = actual_values[:195]\n",
    "\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len_train + window_size, len_train + window_size + 195), actual_values_195, label='Actual Data')\n",
    "plt.plot(range(len_train + window_size, len_train + window_size + 195), predictions_195, label='Predicted Data')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Bandwidth\")\n",
    "plt.title(\"LSTM with Attention Predictions vs Actual Data (First 195 values from 3000 onwards)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
